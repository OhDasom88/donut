resume_from_checkpoint_path: null # only used for resume_from_checkpoint option in PL
result_path: "./result"
pretrained_model_name_or_path: "naver-clova-ix/donut-base" # loading a pre-trained model (from moldehub or path)
# pretrained_model_name_or_path: "/data/models/naver-clova-ix/donut-base" # loading a pre-trained model (from moldehub or path)
# pretrained_model_name_or_path: "/data/models/naver-clova-ix/donut-base/models--naver-clova-ix--donut-base/snapshots/a959cf33c20e09215873e338299c900f57047c6/" # loading a pre-trained model (from moldehub or path)
# dataset_name_or_paths: ["naver-clova-ix/cord-v2"] # loading datasets (from moldehub or path)
dataset_name_or_paths: ["/home/dasom/donut/dataset/hscatalysts/validation"] # loading datasets (from moldehub or path)
# dataset_name_or_paths: ["/data/datasets/naver-clova-ix/cord-v2"] # loading datasets (from moldehub or path)
# dataset_name_or_paths: ["/data/datasets/naver-clova-ix/cord-v2/naver-clova-ix___cord-v2/naver-clova-ix--cord-v2-1b6a08e905758c38/"] # loading datasets (from moldehub or path)
sort_json_key: False # cord dataset is preprocessed, and publicly available at https://huggingface.co/datasets/naver-clova-ix/cord-v2
# train_batch_sizes: [8]
# train_batch_sizes: [1]
train_batch_sizes: [1]
val_batch_sizes: [1]
input_size: [1280, 960] # when the input resolution differs from the pre-training setting, some weights will be newly initialized (but the model training would be okay)
# input_size: [2560, 1920] # when the input resolution differs from the pre-training setting, some weights will be newly initialized (but the model training would be okay)
# max_length: 768
max_length: 4096 # 문서에서 추출해야 하는 토큰수가 더 많음 따라서 더 길게 설정
align_long_axis: False
num_nodes: 1
seed: 2022
lr: 3e-5
warmup_steps: 1000 # 800/8*30/10, 10%
num_training_samples_per_epoch: 800
# max_epochs: 30
max_epochs: 300
max_steps: -1
# num_workers: 8
num_workers: 16
val_check_interval: 1.0
check_val_every_n_epoch: 3
gradient_clip_val: 1.0
verbose: True
wandb_project: "donut-training"
